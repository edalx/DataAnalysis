usableText=str_replace_all(df$text,"[^[:graph:]]", " ")
library(stringr)
usableText=str_replace_all(df$text,"[^[:graph:]]", " ")
usableText
corpus<-Corpus(VectorSource(usableText))
length(corpus)
content(corpus[[15]])
corpus<-tm_map(corpus,tolower)
corpus<-tm_map(corpus,removePunctuation)
corpus<-tm_map(corpus,PlainTextDocument)
content(corpus[[15]])
corpus<-tm_map(corpus,PlainTextDocument)
length(corpus)
content(corpus[[15]])
corpus<-Corpus(VectorSource(usableText))
length(corpus)
content(corpus[[15]])
corpus<-tm_map(corpus,tolower)
corpus<-tm_map(corpus,PlainTextDocument)
content(corpus[[15]])
corpus$content
usableText=str_replace_all(df$text,"[^[:graph:]]", " ")
corpus<-Corpus(VectorSource(usableText))
length(corpus)
corpus$content
corpus<-tm_map(corpus,tolower)
content(corpus[[15]])
corpus<-tm_map(corpus,PlainTextDocument)
corpus<-tm_map(corpus,tolower)
content(corpus[[15]])
corpus<-Corpus(VectorSource(usableText))
length(corpus)
content(corpus[[15]])
corpus<-tm_map(corpus,tolower)
corpus<-tm_map(corpus, removePunctuation)
content(corpus[[15]])
stopwords("espanish")
stopwords("spanish")
corpus<-tm_map(corpus, removeWords, c(stopwords("spanish")))
content(corpus[[15]])
txtclean=str_replace_all(df$text,"[^[:graph:]]", " ")
txtclean = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", txtclean)
txtclean
txtclean = gsub("@\\w+", "", txtclean)
txtclean = gsub("[[:punct:]]", "", txtclean)
# remueve links
txtclean = gsub("http\\w+", "", txtclean)
txtclean
?gsub
txtclean = gsub("[:punct:]", "", txtclean)
txtclean = gsub("http\\w+", "", txtclean)
txtclean
txtclean=str_replace_all(df$text,"[^[:graph:]]", " ")
txtclean
dataTest
df$text
txtclean=str_replace_all(df$text,"[^[:graph:]]", " ")
txtclean
head(txtclean)
txtclean = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", txtclean)
head(txtclean)
txtclean = gsub("[:punct:]", "", txtclean)
head(txtclean)
txtclean=str_replace_all(df$text,"[^[:graph:]]", " ")
txtclean = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", txtclean)
txtclean = gsub("@\\w+", "", txtclean)
head(txtclean)
txtclean = gsub("[[:punct:]]", "", txtclean)
head(txtclean)
txtclean = gsub("http\\w+", "", txtclean)
head(txtclean)
txtclean
df
df$text
ubisoft
?searchTwitter()
txtclean=str_replace_all(df$text,"[^[:graph:]]", " ")
head(txtclean)
txtclean = gsub("[[:graph:]]", "", txtclean)
head(txtclean)
txtclean=str_replace_all(df$text,"[^[:graph:]]", " ")
head(txtclean)
df$text
txtclean = gsub("[[:graph:]]", "", df$text)
head(txtclean)
txtclean=str_replace_all(df$text,"[^[:graph:]]", " ")
head(txtclean)
txtclean = gsub("[:graph:]", "", df$text)
head(txtclean)
txtclean=str_replace_all(df$text,"[^[:graph:]]", " ")
txtclean = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", txtclean)
txtclean = gsub("@\\w+", "", txtclean)
# remueve simbolos de puntuación
txtclean = gsub("[[:punct:]]", "", txtclean)
# remueve links
txtclean = gsub("http\\w+", "", txtclean)
head(txtclean)
corpus<-Corpus(VectorSource(usableText))
length(corpus)
content(corpus[[15]])
corpus<-tm_map(corpus,tolower)
corpus<-tm_map(corpus, removePunctuation)
corpus<-tm_map(corpus, removeWords, c(stopwords("spanish")))
content(corpus[[15]])
corpus = tm_map(corpus, stripWhitespace)
content(corpus[[15]])
txtclean=str_replace_all(df$text,"[^[:graph:]]", " ")
# remueve retweets
txtclean = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", txtclean)
# remove @otragente
txtclean = gsub("@\\w+", "", txtclean)
# remueve simbolos de puntuación
txtclean = gsub("[[:punct:]]", "", txtclean)
# remueve links
txtclean = gsub("http\\w+", "", txtclean)
txtclean = gsub("https\\w+", "", txtclean)
corpus<-Corpus(VectorSource(usableText))
length(corpus)
content(corpus[[15]])
txtclean = gsub("http:\\w+", "", txtclean)
txtclean = gsub("https:\\w+", "", txtclean)
txtclean
txtclean=str_replace_all(df$text,"[^[:graph:]]", " ")
# remueve retweets
txtclean = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", txtclean)
# remove @otragente
txtclean = gsub("@\\w+", "", txtclean)
# remueve simbolos de puntuación
txtclean = gsub("[[:punct:]]", "", txtclean)
# remueve links
txtclean = gsub("http:\\w+", "", txtclean)
txtclean = gsub("https:\\w+", "", txtclean)
corpus<-Corpus(VectorSource(usableText))
length(corpus)
content(corpus[[15]])
corpus<-tm_map(corpus,tolower)
corpus<-tm_map(corpus, removePunctuation)
corpus<-tm_map(corpus, removeWords, c(stopwords("spanish")))
# remove espacios en blanco extras
corpus = tm_map(corpus, stripWhitespace)
content(corpus[[15]])
corpus<-tm_map(corpus,stemDocument)
content(corpus[[15]])
txtclean=str_replace_all(df$text,"[^[:graph:]]", " ")
# remueve retweets
txtclean = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", txtclean,ignore.case = T)
# remove @otragente
txtclean = gsub("@\\w+", "", txtclean)
# remueve simbolos de puntuación
txtclean = gsub("[[:punct:]]", "", txtclean)
# remueve links
txtclean = gsub("http:\\w+", "", txtclean)
txtclean = gsub("https:\\w+", "", txtclean)
head(txtclean)
corpus<-Corpus(VectorSource(usableText))
length(corpus)
content(corpus[[15]])
corpus<-Corpus(VectorSource(txtclean))
length(corpus)
content(corpus[[15]])
corpus<-tm_map(corpus,tolower)
corpus<-tm_map(corpus, removePunctuation)
corpus<-tm_map(corpus, removeWords, c(stopwords("spanish")))
corpus = tm_map(corpus, stripWhitespace)
lema<-tm_map(corpus,stemDocument)
lema
content(lema[[15]])
install.packages("XML")
require( XML )
lematiza <- function( frase ){
palabra <- gsub( " ", "+", frase )
base.url <- paste(
"http://cartago.lllf.uam.es/grampal/grampal.cgi?m=etiqueta&e=",
palabra, sep = "" )
tmp <- readLines( base.url, encoding = 'utf8' )
tmp <- iconv( tmp, "utf-8" )
tmp <- gsub( "&nbsp;", " ", tmp )
tmp <- readHTMLTable( tmp )
tmp <- as.character( tmp[[1]]$V3 )
tmp <- do.call( rbind, strsplit( tmp, " " ) )[,4]
tmp
}
df<-lematiza(df$text)
texto<-lematiza(df$text[1])
texto
df$text[1]
texto<-lematiza(txtclean)
texto<-lematiza(txtclean[1])
texto
txtclean[1]
lematiza("Estan los servidores caídos No consigo hacer login en Uplay")
lematiza("Estan los servidores")
lematiza("Están lavando servidores")
lematiza("queremos comer patatas")
require( XML )
lematiza <- function( frase ){
palabra <- gsub( " ", "+", frase )
base.url <- paste(
"http://cartago.lllf.uam.es/grampal/grampal.cgi?m=etiqueta&e=",
palabra, sep = "" )
tmp <- readLines( base.url, encoding = 'utf8' )
tmp <- iconv( tmp, "utf-8" )
tmp <- gsub( "&nbsp;", " ", tmp )
tmp <- readHTMLTable( tmp )
tmp <- as.character( tmp[[1]]$V3 )
tmp <- do.call( rbind, strsplit( tmp, " " ) )[,4]
tmp
}
lematiza("queremos comer patatas")
corpus<-Corpus(VectorSource(txtclean),readerControl = list(language = "es"))
length(corpus)
content(corpus[[15]])
corpus<-tm_map(corpus,tolower)
corpus<-tm_map(corpus, removePunctuation)
corpus<-tm_map(corpus, removeWords, c(stopwords("spanish")))
# remove espacios en blanco extras
corpus = tm_map(corpus, stripWhitespace)
#lematización
lema<-tm_map(corpus,stemDocument)
content(lema[[15]])
tdm <- TermDocumentMatrix(lema,
control = list(removePunctuation = T,
stopwords = T,
wordLengths = c(2, Inf)))
content(lema[[15]])
tdm
content(tdm[[15]])
tmd[1]
tdm[1]
content(tdm[[1]])
tdm$txtclean
lema<-tm_map(corpus,stemDocument,languageEl="es")
?tm_map
lema<-tm_map(corpus,stemDocument,languageEl=meta(corpus,"spanish"))
lema<-tm_map(stemDocument,languageEl=meta(corpus,"spanish"))
corpus
content(corpus[[15]])
frequencies<-DocumentTermMatrix(corpus)
frequencies
inspect(frequencies[15:20, 5:25])
findFreqTerms(frequencies)
findFreqTerms(frequencies, lowfreq = 20)
findFreqTerms(frequencies, lowfreq = 50)
findFreqTerms(frequencies, lowfreq = 30)
findFreqTerms(frequencies, lowfreq = 40)
findFreqTerms(frequencies, lowfreq = 45)
sparse<-removeSparseTerms(frequencies, 0995)
sparse<-removeSparseTerms(frequencies, 0.995)
sparse
tweetsSparse<-as.data.frame(as.matrix(sparse))
View(tweetsSparse)
tweetsTest<-tweetsSparse[1:20,]
colnames((tweetsTest))=make.names(colnames(tweetsTest))
colnames((tweetsTest))=make.names(colnames(tweetsTest))
library(dplyr)
colnames((tweetsTest))=make.names(colnames(tweetsTest))
colnames(tweetsTest)=make.names(colnames(tweetsTest))
consumer_key="O5rc4ys9mt5sA5tf8AxGvT1Xp"
consumer_secret="0NZyAEBZx8ipK3jhMeCeeqTM7YCTwD3eQke3ceFebBjVWc4ncA"
access_token="564881222-rywwhfbaFNDZGkLBrqkojgkiZeByWNGCJ6GUPTxe"
access_secret="5Y2XamHUK0pFECccYpjU5OebZNzoTGhCbsun2kZ76Q7ep"
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
ubisoft = searchTwitter("@Ubisoft_Spain", n=20, lang="es",)
ubisoft = searchTwitter("@Ubisoft_Spain", n=100, lang="es")
df = twListToDF(ubisoft)
sentimiento=vector(mode = 'character', length = 20)
twitterTest <- sample(100, 20)
dataPredict<-df$text[-twitterTest]
dataTest <- df$text[twitterTest]
length(dataTest)
length(dataPredict)
dataTest
eventos.e3 = searchTwitter("#E32007", n=100, lang="es")
eventos.e3 = searchTwitter("#E32017", n=100, lang="es")
eventos.e3
df = twListToDF(eventos.e3)
twitterTest <- sample(100, 20)
dataTest <- df$text[twitterTest]
dataPredict<-df$text[-twitterTest]
length(dataTest)
length(dataPredict)
sentimiento[1]<-"1"
dataTest
sentimiento[1]<-"1"
sentimiento[2]<-"1"
sentimiento[3]<-"1"
sentimiento[4]<-"1"
sentimiento[5]<-"1"
sentimiento[6]<-"0"
sentimiento[7]<-"0"
sentimiento[8]<-"0"
sentimiento[9]<-"-1"
sentimiento[10]<-"1"
sentimiento[11]<-"1"
sentimiento[12]<-"1"
sentimiento[13]<-"0"
sentimiento[14]<-"0"
sentimiento[15]<-"1"
sentimiento[16]<-"0"
sentimiento[17]<-"0"
sentimiento[18]<-"0"
sentimiento[19]<-"1"
sentimiento[20]<-"1"
txtclean=str_replace_all(dataTest,"[^[:graph:]]", " ")
txtclean = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", txtclean, ignore.case = T)
# remove @otragente
txtclean = gsub("@\\w+", "", txtclean)
# remueve simbolos de puntuación
txtclean = gsub("[[:punct:]]", "", txtclean)
# remueve links
txtclean = gsub("http:\\w+", "", txtclean)
txtclean = gsub("https:\\w+", "", txtclean)
head(txtclean)
corpus<-Corpus(VectorSource(txtclean),readerControl = list(language = "es"))
length(corpus)
content(corpus[[15]])
corpus<-tm_map(corpus,tolower)
corpus<-tm_map(corpus, removePunctuation)
corpus<-tm_map(corpus, removeWords, c(stopwords("spanish")))
# remove espacios en blanco extras
corpus = tm_map(corpus, stripWhitespace)
#Convierte los tweets en matrices 1 0 con las palabras que aparecen en cada uno de ellos
frequencies<-DocumentTermMatrix(corpus)
inspect(frequencies[15:20, 5:25])
findFreqTerms(frequencies, lowfreq = 40)
findFreqTerms(frequencies, lowfreq = 30)
findFreqTerms(frequencies, lowfreq = 20)
findFreqTerms(frequencies, lowfreq = 10)
findFreqTerms(frequencies, lowfreq = 5)
findFreqTerms(frequencies, lowfreq = 3)
findFreqTerms(frequencies, lowfreq = 2)
sparse<-removeSparseTerms(frequencies, 0.995)
sparse
View(tweetsSparse)
length(corpus)
inspect(frequencies[15:20, 5:25])
inspect(frequencies[15:50, 5:25])
inspect(frequencies[1:21, 5:25])
inspect(frequencies[1:20, 5:25])
inspect(frequencies[1:20, 5:25])
inspect(frequencies[1:20, 1:25])
inspect(frequencies[1:20, 1:35])
inspect(frequencies[1:20,])
findFreqTerms(frequencies, lowfreq = 1)
findFreqTerms(frequencies, lowfreq = 2)
sparse<-removeSparseTerms(frequencies, 0.6)
sparse
tweetsSparse<-as.data.frame(as.matrix(sparse))
View(tweetsSparse)
View(tweetsSparse)
sparse<-removeSparseTerms(frequencies, 0.2)
sparse
sparse<-removeSparseTerms(frequencies, 0.95)
sparse
?removeSparseTerms
sparse<-removeSparseTerms(frequencies, 0.96)
sparse
tweetsSparse<-as.data.frame(as.matrix(sparse))
View(tweetsTest)
colnames(tweetsTest)=make.names(colnames(tweetsTest))
tweetsTest$sentimiento<-sentimiento
inspect(frequencies[1:20,])
findFreqTerms(frequencies, lowfreq = 2)
sparse<-removeSparseTerms(frequencies, 0.95)
sparse
tweetsSparse<-as.data.frame(as.matrix(sparse))
View(tweetsSparse)
colnames(tweetsTest)=make.names(colnames(tweetsTest))
colnames(dataTest)=make.names(colnames(dataTest))
colnames(tweetsSparse)=make.names(colnames(tweetsSparse))
dataTest$sentimiento<-sentimiento
tweetsSparse$sentimiento<-sentimiento
tweetsSparse$sentimiento
tweetsSparse
table(tweetsSparse$sentimiento)
11/20
library(e1071)
model <- naiveBayes(as.factor(sentimiento) ~ ., data = tweetsSparse)
summary(model)
predictBayes<-predict(model, dataPredict)
summary(predictBayes)
predictBayes<-predict(model, dataTest)
summary(predictBayes)
predictBayes<-predict(model, tweetsSparse)
summary(predictBayes)
limpiar<-function(datos){
txtclean=str_replace_all(datos,"[^[:graph:]]", " ")
# remueve retweets
txtclean = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", txtclean, ignore.case = T)
# remove @otragente
txtclean = gsub("@\\w+", "", txtclean)
# remueve simbolos de puntuación
txtclean = gsub("[[:punct:]]", "", txtclean)
# remueve links
txtclean = gsub("http:\\w+", "", txtclean)
txtclean = gsub("https:\\w+", "", txtclean)
corpus<-Corpus(VectorSource(txtclean),readerControl = list(language = "es"))
length(corpus)
content(corpus[[15]])
corpus<-tm_map(corpus,tolower)
corpus<-tm_map(corpus, removePunctuation)
corpus<-tm_map(corpus, removeWords, c(stopwords("spanish")))
# remove espacios en blanco extras
corpus = tm_map(corpus, stripWhitespace)
#Convierte los tweets en matrices 1 0 con las palabras que aparecen en cada uno de ellos
frequencies<-DocumentTermMatrix(corpus)
inspect(frequencies[1:20,])
#Encontramos las palabras que se mencionan al menos 2 veces en los tweets
findFreqTerms(frequencies, lowfreq = 2)
#Quitamos las palabras que menos se mencionan en los tweets
sparse<-removeSparseTerms(frequencies, 0.95)
sparse
tweetsSparse<-as.data.frame(as.matrix(sparse))
}
limpiar<-function(datos){
txtclean=str_replace_all(datos,"[^[:graph:]]", " ")
# remueve retweets
txtclean = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", txtclean, ignore.case = T)
# remove @otragente
txtclean = gsub("@\\w+", "", txtclean)
# remueve simbolos de puntuación
txtclean = gsub("[[:punct:]]", "", txtclean)
# remueve links
txtclean = gsub("http:\\w+", "", txtclean)
txtclean = gsub("https:\\w+", "", txtclean)
corpus<-Corpus(VectorSource(txtclean),readerControl = list(language = "es"))
length(corpus)
content(corpus[[15]])
corpus<-tm_map(corpus,tolower)
corpus<-tm_map(corpus, removePunctuation)
corpus<-tm_map(corpus, removeWords, c(stopwords("spanish")))
# remove espacios en blanco extras
corpus = tm_map(corpus, stripWhitespace)
#Convierte los tweets en matrices 1 0 con las palabras que aparecen en cada uno de ellos
frequencies<-DocumentTermMatrix(corpus)
inspect(frequencies[1:20,])
#Encontramos las palabras que se mencionan al menos 2 veces en los tweets
findFreqTerms(frequencies, lowfreq = 2)
#Quitamos las palabras que menos se mencionan en los tweets
sparse<-removeSparseTerms(frequencies, 0.95)
sparse
tweetsSparse<-as.data.frame(as.matrix(sparse))
tweetsSparse
}
pDatos<-limpiar(dataPredict)
pDatos
predictBayes<-predict(model, pDatos)
summary
predictBayes
dataPredict[3]
dataPredict[0]
dataPredict[1]
dataPredict[2]
dataPredict[59]
getwd()
getwd("C:/Users/edd_a/Documents/DataAnalysis")
setwd("C:/Users/edd_a/Documents/DataAnalysis")
getwd()
library(twitteR)
library(base64enc)
library(NLP)
library(tm)
library(SnowballC)
library(stringr)
library(dplyr)
library(e1071)
consumer_key="O5rc4ys9mt5sA5tf8AxGvT1Xp"
consumer_secret="0NZyAEBZx8ipK3jhMeCeeqTM7YCTwD3eQke3ceFebBjVWc4ncA"
access_token="564881222-rywwhfbaFNDZGkLBrqkojgkiZeByWNGCJ6GUPTxe"
access_secret="5Y2XamHUK0pFECccYpjU5OebZNzoTGhCbsun2kZ76Q7ep"
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
eventos.e3 <- searchTwitter("#E32017",since="2017-05-01",until='2017-07-17', lang = "es")
eventos.e3 <- searchTwitter("#E32017",n=1000,since="2017-05-01",until='2017-07-17', lang = "es")
eventos.xbox<- searchTwitter("#XboxE3", n=1000,since="2017-05-01",until='2017-07-12', lang = "es")
eventos.nintendo<-searchTwitter("#NintendoE3", n=1000,since="2017-05-01",until='2017-07-12', lang = "es")
eventos.playstation<-searchTwitter("#SonyE3", n=1000,since="2017-05-01",until='2017-07-12', lang = "es")
marcas.xbox <- searchTwitter('Xbox_Spain',n=1000,since="2017-05-01",until='2017-07-12', lang = "es")
marcas.nintendo <- searchTwitter('NintendoES',n=1000,since="2017-05-01",until='2017-07-12', lang = "es")
marcas.playstation <-searchTwitter("PlayStationES",n=1000,since="2017-05-01",until='2017-07-12',lang = "es")
marcas.ubisoft <-searchTwitter("Ubisoft_Spain",n=1000,since="2017-05-01",until='2017-07-12', lang = "es")
marcas.ElectronicArts <-searchTwitter("EA_Espana",n=1000,since="2017-05-01",until='2017-07-12', lang = "es")
marcas.Betsheda <-searchTwitter("bethesda_ESP",n=1000,since="2017-05-01",until='2017-07-12', lang = "es")
marcas.IGN <-searchTwitter("IGN_es",n=1000,since="2017-05-01",until='2017-07-12', lang = "es")
marcas.3DJuegos <-searchTwitter("3djuegos",n=1000,since="2017-05-01",until='2017-07-12', lang = "es")
marcas.MundoGamers <-searchTwitter("Mundogamers",n=1000,since="2017-05-01",until='2017-07-12', lang = "es")
marcas.VandalOnline <-searchTwitter("VandalOnline",n=1000,since="2017-05-01",until='2017-07-12', lang = "es")
doInstall <- TRUE
toInstall <- c("twitteR", "dismo", "maps", "ggplot2")
if(doInstall){install.packages(toInstall, repos = "http://cran.us.r-project.org")}
lapply(toInstall, library, character.only = TRUE)
colnames(locations)
searchTerm <- "#sismo" # termino de busqueda
searchResults <- searchTwitter(searchTerm, n = 100) # Búsqueda
tweetFrame <- twListToDF(searchResults) # Creamos data frame
userInfo <- lookupUsers(tweetFrame$screenName) #Información del usuario (Nombre)
userFrame <- twListToDF(userInfo) # Creamos data frame
locatedUsers <- !is.na(userFrame$location) #Mantenga sólo los usuarios con información de ubicación
locations <- geocode(userFrame$location[locatedUsers]) #API de adivinar Aproximada lat / lon de datos de ubicación textual.
locations
colnames(locations)
searchTerm <- "#sismo" # termino de busqueda
searchResults <- searchTwitter(searchTerm, n = 100) # Búsqueda
consumer_key="O5rc4ys9mt5sA5tf8AxGvT1Xp"
consumer_secret="0NZyAEBZx8ipK3jhMeCeeqTM7YCTwD3eQke3ceFebBjVWc4ncA"
access_token="564881222-rywwhfbaFNDZGkLBrqkojgkiZeByWNGCJ6GUPTxe"
access_secret="5Y2XamHUK0pFECccYpjU5OebZNzoTGhCbsun2kZ76Q7ep"
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
consumer_key="O5rc4ys9mt5sA5tf8AxGvT1Xp"
consumer_secret="0NZyAEBZx8ipK3jhMeCeeqTM7YCTwD3eQke3ceFebBjVWc4ncA"
access_token="564881222-rywwhfbaFNDZGkLBrqkojgkiZeByWNGCJ6GUPTxe"
access_secret="5Y2XamHUK0pFECccYpjU5OebZNzoTGhCbsun2kZ76Q7ep"
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
